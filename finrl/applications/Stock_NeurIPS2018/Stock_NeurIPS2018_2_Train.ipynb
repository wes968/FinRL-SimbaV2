{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [],
   "source": [
    "## install required packages\n",
    "#!pip install swig\n",
    "#!pip install wrds\n",
    "#!pip install pyportfolioopt\n",
    "## install finrl library\n",
    "#!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl import config_tickers\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 111         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | -0.443      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -16.5       |\n",
      "|    reward             | -0.29590988 |\n",
      "|    reward_max         | 0.80994046  |\n",
      "|    reward_mean        | 0.23194848  |\n",
      "|    reward_min         | -0.5028118  |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 1.12        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 111        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.00688   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -42.8      |\n",
      "|    reward             | -1.5739539 |\n",
      "|    reward_max         | 2.0882154  |\n",
      "|    reward_mean        | 0.46113253 |\n",
      "|    reward_min         | -1.5739539 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 1.29       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 112         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -175        |\n",
      "|    reward             | 7.311467    |\n",
      "|    reward_max         | 7.311467    |\n",
      "|    reward_mean        | 0.033735324 |\n",
      "|    reward_min         | -4.207945   |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 19.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 113        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -20.1      |\n",
      "|    reward             | 1.1948131  |\n",
      "|    reward_max         | 7.7401943  |\n",
      "|    reward_mean        | 2.202359   |\n",
      "|    reward_min         | 0.25737754 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.87       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 115        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 2.38e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 821        |\n",
      "|    reward             | -15.561892 |\n",
      "|    reward_max         | 9.125257   |\n",
      "|    reward_mean        | -4.790314  |\n",
      "|    reward_min         | -21.859644 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 474        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 116        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.335     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 176        |\n",
      "|    reward             | 0.27889773 |\n",
      "|    reward_max         | 3.1773071  |\n",
      "|    reward_mean        | 0.5296326  |\n",
      "|    reward_min         | -0.5078438 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 22.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 117         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -199        |\n",
      "|    reward             | -3.3488827  |\n",
      "|    reward_max         | 2.174704    |\n",
      "|    reward_mean        | 0.010154354 |\n",
      "|    reward_min         | -3.3488827  |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 24.8        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 117         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 33          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -116        |\n",
      "|    reward             | -0.56865364 |\n",
      "|    reward_max         | 1.8214476   |\n",
      "|    reward_mean        | -0.16498011 |\n",
      "|    reward_min         | -1.3373501  |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 8.95        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 118         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 37          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 328         |\n",
      "|    reward             | -2.821166   |\n",
      "|    reward_max         | 1.501792    |\n",
      "|    reward_mean        | -0.18967944 |\n",
      "|    reward_min         | -2.821166   |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 81.9        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 118         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 42          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -22.8       |\n",
      "|    reward             | -4.301261   |\n",
      "|    reward_max         | -0.94662917 |\n",
      "|    reward_mean        | -2.8685231  |\n",
      "|    reward_min         | -5.5915995  |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.93        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 119       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -242      |\n",
      "|    reward             | 5.810378  |\n",
      "|    reward_max         | 7.683316  |\n",
      "|    reward_mean        | 1.1833694 |\n",
      "|    reward_min         | -8.315171 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 39.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 119         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0.0741      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -178        |\n",
      "|    reward             | -0.15231875 |\n",
      "|    reward_max         | 3.6519458   |\n",
      "|    reward_mean        | 0.9785046   |\n",
      "|    reward_min         | -0.15231875 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 22.1        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 119         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 54          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.00185    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 80.4        |\n",
      "|    reward             | -4.090373   |\n",
      "|    reward_max         | 4.2965007   |\n",
      "|    reward_mean        | -0.39737597 |\n",
      "|    reward_min         | -4.090373   |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 16.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 119        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 98.9       |\n",
      "|    reward             | 0.9743293  |\n",
      "|    reward_max         | 2.0440345  |\n",
      "|    reward_mean        | 0.63220775 |\n",
      "|    reward_min         | -1.2058043 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 62         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0101     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -58.5      |\n",
      "|    reward             | 2.1185586  |\n",
      "|    reward_max         | 6.797147   |\n",
      "|    reward_mean        | 2.0029917  |\n",
      "|    reward_min         | -0.8882651 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 26.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 159        |\n",
      "|    reward             | 1.8026307  |\n",
      "|    reward_max         | 1.8026307  |\n",
      "|    reward_mean        | 0.434363   |\n",
      "|    reward_min         | -1.2311958 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 23.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -113       |\n",
      "|    reward             | 5.975807   |\n",
      "|    reward_max         | 5.975807   |\n",
      "|    reward_mean        | 2.7878873  |\n",
      "|    reward_min         | -2.7203276 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 20.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 120         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 74          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | -0.0268     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -64.2       |\n",
      "|    reward             | 0.31086293  |\n",
      "|    reward_max         | 0.5598118   |\n",
      "|    reward_mean        | 0.19359006  |\n",
      "|    reward_min         | -0.31766784 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.4         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 120         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 164         |\n",
      "|    reward             | -0.40721834 |\n",
      "|    reward_max         | 1.5345942   |\n",
      "|    reward_mean        | 0.085969396 |\n",
      "|    reward_min         | -2.0086207  |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 20.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -96.8      |\n",
      "|    reward             | 2.2752855  |\n",
      "|    reward_max         | 3.2522485  |\n",
      "|    reward_mean        | 1.6443179  |\n",
      "|    reward_min         | -1.2788502 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.85       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 120       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 87        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 63.5      |\n",
      "|    reward             | 4.128302  |\n",
      "|    reward_max         | 4.128302  |\n",
      "|    reward_mean        | 1.1653688 |\n",
      "|    reward_min         | -1.345456 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 12.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 119       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | -23.6     |\n",
      "|    reward             | -9.162328 |\n",
      "|    reward_max         | 1.9493726 |\n",
      "|    reward_mean        | -4.649481 |\n",
      "|    reward_min         | -9.162328 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 15.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 119        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -3.45e+03  |\n",
      "|    reward             | 2.5149226  |\n",
      "|    reward_max         | 44.75647   |\n",
      "|    reward_mean        | -1.6006348 |\n",
      "|    reward_min         | -67.05171  |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.85e+03   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 119         |\n",
      "|    iterations         | 2400        |\n",
      "|    time_elapsed       | 100         |\n",
      "|    total_timesteps    | 12000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2399        |\n",
      "|    policy_loss        | 67.9        |\n",
      "|    reward             | 1.5573063   |\n",
      "|    reward_max         | 1.5573063   |\n",
      "|    reward_mean        | 0.5994337   |\n",
      "|    reward_min         | -0.52061844 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.5         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 119         |\n",
      "|    iterations         | 2500        |\n",
      "|    time_elapsed       | 104         |\n",
      "|    total_timesteps    | 12500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | -0.0812     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2499        |\n",
      "|    policy_loss        | -89.3       |\n",
      "|    reward             | 0.0453568   |\n",
      "|    reward_max         | 3.6909811   |\n",
      "|    reward_mean        | 1.085815    |\n",
      "|    reward_min         | -0.42165434 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.95        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 119        |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | -17.6      |\n",
      "|    reward             | 2.2532377  |\n",
      "|    reward_max         | 2.2532377  |\n",
      "|    reward_mean        | 0.71757877 |\n",
      "|    reward_min         | -1.0768068 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.345      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 120         |\n",
      "|    iterations         | 2700        |\n",
      "|    time_elapsed       | 112         |\n",
      "|    total_timesteps    | 13500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0.279       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2699        |\n",
      "|    policy_loss        | -35         |\n",
      "|    reward             | 0.22166486  |\n",
      "|    reward_max         | 2.2611592   |\n",
      "|    reward_mean        | -0.19774672 |\n",
      "|    reward_min         | -1.6554092  |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.28        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 116        |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 336        |\n",
      "|    reward             | 2.3906884  |\n",
      "|    reward_max         | 2.3906884  |\n",
      "|    reward_mean        | 0.22372827 |\n",
      "|    reward_min         | -2.0486484 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 78.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | -175       |\n",
      "|    reward             | 5.0950184  |\n",
      "|    reward_max         | 5.0950184  |\n",
      "|    reward_mean        | 0.26695728 |\n",
      "|    reward_min         | -2.642264  |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 21.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 120         |\n",
      "|    iterations         | 3000        |\n",
      "|    time_elapsed       | 124         |\n",
      "|    total_timesteps    | 15000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2999        |\n",
      "|    policy_loss        | 86.8        |\n",
      "|    reward             | 0.9472115   |\n",
      "|    reward_max         | 0.9472115   |\n",
      "|    reward_mean        | 0.045377303 |\n",
      "|    reward_min         | -0.48915267 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 5.07        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 120         |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 128         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 62.4        |\n",
      "|    reward             | -0.27101114 |\n",
      "|    reward_max         | 1.8347007   |\n",
      "|    reward_mean        | 0.5089384   |\n",
      "|    reward_min         | -0.3044031  |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.76        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 120       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 57.9      |\n",
      "|    reward             | -1.498552 |\n",
      "|    reward_max         | 5.3840895 |\n",
      "|    reward_mean        | 1.6933444 |\n",
      "|    reward_min         | -1.498552 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 34.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 136        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | 447        |\n",
      "|    reward             | -2.6181042 |\n",
      "|    reward_max         | 0.85126495 |\n",
      "|    reward_mean        | -2.9884806 |\n",
      "|    reward_min         | -6.4622293 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 114        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 120       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 133       |\n",
      "|    reward             | 12.083977 |\n",
      "|    reward_max         | 12.083977 |\n",
      "|    reward_mean        | 2.2346117 |\n",
      "|    reward_min         | -8.490407 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 120       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 145        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0579     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 143        |\n",
      "|    reward             | -0.7632348 |\n",
      "|    reward_max         | 3.7256634  |\n",
      "|    reward_mean        | 0.38675818 |\n",
      "|    reward_min         | -0.7632348 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 18.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 149        |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -160       |\n",
      "|    reward             | 0.43663022 |\n",
      "|    reward_max         | 2.0648346  |\n",
      "|    reward_mean        | 0.19614114 |\n",
      "|    reward_min         | -1.6542116 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 20.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 153        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0302     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | 300        |\n",
      "|    reward             | 0.65039617 |\n",
      "|    reward_max         | 0.9875646  |\n",
      "|    reward_mean        | 0.2942137  |\n",
      "|    reward_min         | -0.5535796 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 57.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 157        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 129        |\n",
      "|    reward             | 0.950817   |\n",
      "|    reward_max         | 8.19198    |\n",
      "|    reward_mean        | 3.4091277  |\n",
      "|    reward_min         | -5.5441833 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 13.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 161        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | -212       |\n",
      "|    reward             | 2.8525465  |\n",
      "|    reward_max         | 2.8525465  |\n",
      "|    reward_mean        | 0.1586231  |\n",
      "|    reward_min         | -4.9103727 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 28.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 120       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -296      |\n",
      "|    reward             | 3.4832635 |\n",
      "|    reward_max         | 5.71763   |\n",
      "|    reward_mean        | 3.7351503 |\n",
      "|    reward_min         | 2.2049668 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 59        |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 120         |\n",
      "|    iterations         | 4100        |\n",
      "|    time_elapsed       | 169         |\n",
      "|    total_timesteps    | 20500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | -0.0222     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4099        |\n",
      "|    policy_loss        | -47.8       |\n",
      "|    reward             | 0.073971674 |\n",
      "|    reward_max         | 1.4374949   |\n",
      "|    reward_mean        | 0.5025862   |\n",
      "|    reward_min         | -0.07477635 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.53        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 120         |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 173         |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | -155        |\n",
      "|    reward             | -0.11567606 |\n",
      "|    reward_max         | 5.2164636   |\n",
      "|    reward_mean        | 1.057655    |\n",
      "|    reward_min         | -2.413899   |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 16          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 177        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | -121       |\n",
      "|    reward             | 5.421511   |\n",
      "|    reward_max         | 5.421511   |\n",
      "|    reward_mean        | 1.228928   |\n",
      "|    reward_min         | -1.0474828 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 181        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.0815     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | 21.1       |\n",
      "|    reward             | 2.1730697  |\n",
      "|    reward_max         | 2.1730697  |\n",
      "|    reward_mean        | -2.1356282 |\n",
      "|    reward_min         | -7.804102  |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 11.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 186        |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 729        |\n",
      "|    reward             | 1.2489575  |\n",
      "|    reward_max         | 1.2489575  |\n",
      "|    reward_mean        | -1.0912273 |\n",
      "|    reward_min         | -3.6589925 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 358        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 190        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 1.25e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 198        |\n",
      "|    reward             | 5.9384923  |\n",
      "|    reward_max         | 5.9384923  |\n",
      "|    reward_mean        | -1.2477002 |\n",
      "|    reward_min         | -6.897651  |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 37.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 194        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -175       |\n",
      "|    reward             | 0.8924312  |\n",
      "|    reward_max         | 5.795514   |\n",
      "|    reward_mean        | 1.4934257  |\n",
      "|    reward_min         | -1.8927338 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 28         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 198        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -133       |\n",
      "|    reward             | -0.4902786 |\n",
      "|    reward_max         | 3.4531083  |\n",
      "|    reward_mean        | 0.5008698  |\n",
      "|    reward_min         | -0.4902786 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 18.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 202        |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.0195    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | 13.3       |\n",
      "|    reward             | 1.8519806  |\n",
      "|    reward_max         | 4.287058   |\n",
      "|    reward_mean        | 0.8485905  |\n",
      "|    reward_min         | -3.9071093 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 5.1        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 5000        |\n",
      "|    time_elapsed       | 206         |\n",
      "|    total_timesteps    | 25000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4999        |\n",
      "|    policy_loss        | 75.3        |\n",
      "|    reward             | -0.21657611 |\n",
      "|    reward_max         | 3.4365509   |\n",
      "|    reward_mean        | -0.31340617 |\n",
      "|    reward_min         | -3.3261955  |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 9.37        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 210        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.00973   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 275        |\n",
      "|    reward             | -5.5558343 |\n",
      "|    reward_max         | 7.2351766  |\n",
      "|    reward_mean        | 1.8049873  |\n",
      "|    reward_min         | -5.5558343 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 51.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 214       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -573      |\n",
      "|    reward             | 21.774078 |\n",
      "|    reward_max         | 21.774078 |\n",
      "|    reward_mean        | 6.3558974 |\n",
      "|    reward_min         | -5.478912 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 217       |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7269844.68\n",
      "total_reward: 6269844.68\n",
      "total_cost: 67518.47\n",
      "total_trades: 52533\n",
      "Sharpe: 0.979\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 5300        |\n",
      "|    time_elapsed       | 218         |\n",
      "|    total_timesteps    | 26500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5299        |\n",
      "|    policy_loss        | 50.8        |\n",
      "|    reward             | 0.77135533  |\n",
      "|    reward_max         | 2.3938365   |\n",
      "|    reward_mean        | -0.29254204 |\n",
      "|    reward_min         | -4.5894227  |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.29        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 222        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.00888    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -133       |\n",
      "|    reward             | 1.4432204  |\n",
      "|    reward_max         | 1.4432204  |\n",
      "|    reward_mean        | -1.3053025 |\n",
      "|    reward_min         | -4.4035234 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 226        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 290        |\n",
      "|    reward             | 8.485796   |\n",
      "|    reward_max         | 9.716447   |\n",
      "|    reward_mean        | 4.467516   |\n",
      "|    reward_min         | -5.9007144 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 57.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 231        |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | -83.1      |\n",
      "|    reward             | 0.67930144 |\n",
      "|    reward_max         | 7.3304887  |\n",
      "|    reward_mean        | 0.29448062 |\n",
      "|    reward_min         | -2.9713576 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 16.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 235        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -704       |\n",
      "|    reward             | -8.257394  |\n",
      "|    reward_max         | 18.16398   |\n",
      "|    reward_mean        | -7.696352  |\n",
      "|    reward_min         | -35.786205 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 321        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 239       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.00423  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -9.34     |\n",
      "|    reward             | 3.8278596 |\n",
      "|    reward_max         | 6.8620954 |\n",
      "|    reward_mean        | 1.321712  |\n",
      "|    reward_min         | -6.560555 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.81      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 243         |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | 59.4        |\n",
      "|    reward             | -0.20918421 |\n",
      "|    reward_max         | 1.2869239   |\n",
      "|    reward_mean        | 0.1698667   |\n",
      "|    reward_min         | -0.56268567 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.02        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 247        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 73.1       |\n",
      "|    reward             | -1.1900449 |\n",
      "|    reward_max         | 1.1948589  |\n",
      "|    reward_mean        | -0.6114246 |\n",
      "|    reward_min         | -3.3069491 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 251        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | -39        |\n",
      "|    reward             | -2.630903  |\n",
      "|    reward_max         | 3.7605042  |\n",
      "|    reward_mean        | -1.3188936 |\n",
      "|    reward_min         | -6.5274043 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 255        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | 106        |\n",
      "|    reward             | 0.21823357 |\n",
      "|    reward_max         | 0.21823357 |\n",
      "|    reward_mean        | -0.6958405 |\n",
      "|    reward_min         | -2.5111604 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.76       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 6300        |\n",
      "|    time_elapsed       | 259         |\n",
      "|    total_timesteps    | 31500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6299        |\n",
      "|    policy_loss        | 288         |\n",
      "|    reward             | 2.9450226   |\n",
      "|    reward_max         | 3.5622175   |\n",
      "|    reward_mean        | -0.75122386 |\n",
      "|    reward_min         | -6.48946    |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 63.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 264        |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.112     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | 187        |\n",
      "|    reward             | 1.8409696  |\n",
      "|    reward_max         | 2.6419525  |\n",
      "|    reward_mean        | 1.0274935  |\n",
      "|    reward_min         | -0.2844737 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 19.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 268        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 29.9       |\n",
      "|    reward             | -5.1674385 |\n",
      "|    reward_max         | 4.5310235  |\n",
      "|    reward_mean        | -1.6564417 |\n",
      "|    reward_min         | -5.1674385 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.75       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 272         |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -61.6       |\n",
      "|    reward             | -0.40089506 |\n",
      "|    reward_max         | 1.576305    |\n",
      "|    reward_mean        | 0.045428712 |\n",
      "|    reward_min         | -1.4486805  |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 12.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 276        |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | -1.37e+03  |\n",
      "|    reward             | -12.955465 |\n",
      "|    reward_max         | 19.539562  |\n",
      "|    reward_mean        | 2.6296952  |\n",
      "|    reward_min         | -12.955465 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.32e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 280        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | -74.5      |\n",
      "|    reward             | 1.7776154  |\n",
      "|    reward_max         | 5.4316053  |\n",
      "|    reward_mean        | 0.875234   |\n",
      "|    reward_min         | -4.8368344 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 22.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 284        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -227       |\n",
      "|    reward             | -2.0266402 |\n",
      "|    reward_max         | 9.140911   |\n",
      "|    reward_mean        | 3.4296834  |\n",
      "|    reward_min         | -5.0919223 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 138        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 288        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 54.8       |\n",
      "|    reward             | 0.1052091  |\n",
      "|    reward_max         | 2.9200244  |\n",
      "|    reward_mean        | 0.6029784  |\n",
      "|    reward_min         | -0.4067069 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.52       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 292         |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | 93          |\n",
      "|    reward             | 0.17449024  |\n",
      "|    reward_max         | 1.7586014   |\n",
      "|    reward_mean        | 0.78684306  |\n",
      "|    reward_min         | 0.017902061 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.53        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 296        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -374       |\n",
      "|    reward             | 0.44462863 |\n",
      "|    reward_max         | 5.055124   |\n",
      "|    reward_mean        | 2.136821   |\n",
      "|    reward_min         | -0.3421222 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 82.4       |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 121           |\n",
      "|    iterations         | 7300          |\n",
      "|    time_elapsed       | 301           |\n",
      "|    total_timesteps    | 36500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -41.5         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7299          |\n",
      "|    policy_loss        | 82.7          |\n",
      "|    reward             | -8.702298     |\n",
      "|    reward_max         | 10.600994     |\n",
      "|    reward_mean        | -0.0068877935 |\n",
      "|    reward_min         | -8.702298     |\n",
      "|    std                | 1.01          |\n",
      "|    value_loss         | 11.8          |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 305        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | 637        |\n",
      "|    reward             | -9.736578  |\n",
      "|    reward_max         | 5.1460533  |\n",
      "|    reward_mean        | -0.5736043 |\n",
      "|    reward_min         | -9.736578  |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 295        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 309        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 729        |\n",
      "|    reward             | -16.603828 |\n",
      "|    reward_max         | 2.5235677  |\n",
      "|    reward_mean        | -6.785376  |\n",
      "|    reward_min         | -16.603828 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 340        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 313       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -75.7     |\n",
      "|    reward             | 2.4819443 |\n",
      "|    reward_max         | 2.6552346 |\n",
      "|    reward_mean        | 1.0774342 |\n",
      "|    reward_min         | -2.301706 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.62      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 317         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | -83.4       |\n",
      "|    reward             | 1.4781617   |\n",
      "|    reward_max         | 2.4893794   |\n",
      "|    reward_mean        | 0.8653836   |\n",
      "|    reward_min         | -0.48392534 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 4.47        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 321        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0.00215    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | -131       |\n",
      "|    reward             | -0.7740982 |\n",
      "|    reward_max         | 3.1956367  |\n",
      "|    reward_mean        | 0.57874125 |\n",
      "|    reward_min         | -2.2489197 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 325        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | 454        |\n",
      "|    reward             | 3.611323   |\n",
      "|    reward_max         | 3.611323   |\n",
      "|    reward_mean        | 1.2616259  |\n",
      "|    reward_min         | -1.3106976 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 133        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 329        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 1.61e-06   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -148       |\n",
      "|    reward             | -3.7319481 |\n",
      "|    reward_max         | 19.281538  |\n",
      "|    reward_mean        | 1.616842   |\n",
      "|    reward_min         | -4.3929296 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 15.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8100       |\n",
      "|    time_elapsed       | 334        |\n",
      "|    total_timesteps    | 40500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8099       |\n",
      "|    policy_loss        | 55.6       |\n",
      "|    reward             | 10.846665  |\n",
      "|    reward_max         | 10.846665  |\n",
      "|    reward_mean        | -3.5213804 |\n",
      "|    reward_min         | -23.817274 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 18.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 338        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -164       |\n",
      "|    reward             | -0.9148212 |\n",
      "|    reward_max         | 1.8068625  |\n",
      "|    reward_mean        | 0.3425116  |\n",
      "|    reward_min         | -0.9148212 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 18         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 342        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 52.5       |\n",
      "|    reward             | -1.9688175 |\n",
      "|    reward_max         | 1.0781645  |\n",
      "|    reward_mean        | -0.5186563 |\n",
      "|    reward_min         | -1.9688175 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.19       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 346        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -40.7      |\n",
      "|    reward             | -1.9821085 |\n",
      "|    reward_max         | 3.727092   |\n",
      "|    reward_mean        | -1.7991036 |\n",
      "|    reward_min         | -4.377553  |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 350        |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | -179       |\n",
      "|    reward             | -1.6484547 |\n",
      "|    reward_max         | 3.504638   |\n",
      "|    reward_mean        | 0.7346065  |\n",
      "|    reward_min         | -1.6484547 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 23.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 354        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0.00161    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 207        |\n",
      "|    reward             | -5.0381665 |\n",
      "|    reward_max         | 4.479526   |\n",
      "|    reward_mean        | -0.8083691 |\n",
      "|    reward_min         | -5.0381665 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 45.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 358        |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0.000111   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | 88.1       |\n",
      "|    reward             | 3.815704   |\n",
      "|    reward_max         | 3.815704   |\n",
      "|    reward_mean        | 1.1436529  |\n",
      "|    reward_min         | -0.6951168 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 6.21       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 8800        |\n",
      "|    time_elapsed       | 363         |\n",
      "|    total_timesteps    | 44000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.1       |\n",
      "|    explained_variance | 0.00124     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8799        |\n",
      "|    policy_loss        | -61         |\n",
      "|    reward             | 1.1262301   |\n",
      "|    reward_max         | 1.1262301   |\n",
      "|    reward_mean        | -0.14600115 |\n",
      "|    reward_min         | -0.97196776 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.66        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 367        |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | 31.1       |\n",
      "|    reward             | 0.14106297 |\n",
      "|    reward_max         | 0.14106297 |\n",
      "|    reward_mean        | -0.5392532 |\n",
      "|    reward_min         | -1.5460261 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 371        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | -30.8      |\n",
      "|    reward             | -1.7769718 |\n",
      "|    reward_max         | 3.872923   |\n",
      "|    reward_mean        | 0.40317068 |\n",
      "|    reward_min         | -3.1304154 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.28       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 9100        |\n",
      "|    time_elapsed       | 375         |\n",
      "|    total_timesteps    | 45500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9099        |\n",
      "|    policy_loss        | 32.5        |\n",
      "|    reward             | 0.6594702   |\n",
      "|    reward_max         | 0.6594702   |\n",
      "|    reward_mean        | -0.42721713 |\n",
      "|    reward_min         | -1.7185001  |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.26        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 379        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 64.8       |\n",
      "|    reward             | 4.896722   |\n",
      "|    reward_max         | 4.896722   |\n",
      "|    reward_mean        | -0.8764992 |\n",
      "|    reward_min         | -4.391023  |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 5.98       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 9300        |\n",
      "|    time_elapsed       | 383         |\n",
      "|    total_timesteps    | 46500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9299        |\n",
      "|    policy_loss        | -202        |\n",
      "|    reward             | -0.09606907 |\n",
      "|    reward_max         | 3.5633705   |\n",
      "|    reward_mean        | -0.11830667 |\n",
      "|    reward_min         | -5.1418476  |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 24.5        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 387         |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 139         |\n",
      "|    reward             | -0.72350186 |\n",
      "|    reward_max         | 6.065443    |\n",
      "|    reward_mean        | 0.82769907  |\n",
      "|    reward_min         | -4.8971696  |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 13.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 391        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 352        |\n",
      "|    reward             | 0.99193364 |\n",
      "|    reward_max         | 2.942359   |\n",
      "|    reward_mean        | 0.5892754  |\n",
      "|    reward_min         | -1.6097986 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 75.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 395        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -354       |\n",
      "|    reward             | -1.1470324 |\n",
      "|    reward_max         | 2.9840827  |\n",
      "|    reward_mean        | 0.76568294 |\n",
      "|    reward_min         | -1.1470324 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 79.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 399        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 85.8       |\n",
      "|    reward             | -0.6698689 |\n",
      "|    reward_max         | 6.544212   |\n",
      "|    reward_mean        | 1.1350732  |\n",
      "|    reward_min         | -1.381271  |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 6.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 9800       |\n",
      "|    time_elapsed       | 403        |\n",
      "|    total_timesteps    | 49000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9799       |\n",
      "|    policy_loss        | 96.1       |\n",
      "|    reward             | 2.2184758  |\n",
      "|    reward_max         | 3.888274   |\n",
      "|    reward_mean        | 1.2661049  |\n",
      "|    reward_min         | -1.8532292 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 28.5       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 121          |\n",
      "|    iterations         | 9900         |\n",
      "|    time_elapsed       | 407          |\n",
      "|    total_timesteps    | 49500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.2        |\n",
      "|    explained_variance | 0.000342     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9899         |\n",
      "|    policy_loss        | 20.5         |\n",
      "|    reward             | -0.36948925  |\n",
      "|    reward_max         | 0.73940027   |\n",
      "|    reward_mean        | -0.049800675 |\n",
      "|    reward_min         | -0.8281443   |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 0.772        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 10000       |\n",
      "|    time_elapsed       | 412         |\n",
      "|    total_timesteps    | 50000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9999        |\n",
      "|    policy_loss        | -13.6       |\n",
      "|    reward             | -1.099806   |\n",
      "|    reward_max         | 1.3865483   |\n",
      "|    reward_mean        | -0.33318383 |\n",
      "|    reward_min         | -1.5289102  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.422       |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 52        |\n",
      "|    time_elapsed    | 218       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -24.1     |\n",
      "|    critic_loss     | 1.06e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 10.201075 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 53        |\n",
      "|    time_elapsed    | 433       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -37.4     |\n",
      "|    critic_loss     | 18.1      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 23043     |\n",
      "|    reward          | 10.201075 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6983367.50\n",
      "total_reward: 5983367.50\n",
      "total_cost: 999.00\n",
      "total_trades: 26028\n",
      "Sharpe: 1.062\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 54        |\n",
      "|    time_elapsed    | 640       |\n",
      "|    total_timesteps | 34716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -31.8     |\n",
      "|    critic_loss     | 8.71      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 34615     |\n",
      "|    reward          | 10.201075 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 52        |\n",
      "|    time_elapsed    | 884       |\n",
      "|    total_timesteps | 46288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -29.2     |\n",
      "|    critic_loss     | 7.04      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 46187     |\n",
      "|    reward          | 10.201075 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 138         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 14          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.07334936 |\n",
      "|    reward_max      | 9.1277075   |\n",
      "|    reward_mean     | 0.09318155  |\n",
      "|    reward_min      | -9.70133    |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 135        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01641884 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | -0.00175   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 5.71       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    reward               | 0.7576518  |\n",
      "|    reward_max           | 39.69544   |\n",
      "|    reward_mean          | 0.10408766 |\n",
      "|    reward_min           | -48.757057 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 15.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023804639 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00098     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    reward               | -1.8959033  |\n",
      "|    reward_max           | 30.063267   |\n",
      "|    reward_mean          | 0.10447094  |\n",
      "|    reward_min           | -39.93596   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 82.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0139464885 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -0.00575     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 49.6         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    reward               | 4.882779     |\n",
      "|    reward_max           | 11.567445    |\n",
      "|    reward_mean          | 0.13518831   |\n",
      "|    reward_min           | -16.736034   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 66.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018612588 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0192     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    reward               | 2.5624588   |\n",
      "|    reward_max           | 43.971684   |\n",
      "|    reward_mean          | 0.08566452  |\n",
      "|    reward_min           | -52.45188   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 25.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014021803 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00333    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    reward               | 2.0199084   |\n",
      "|    reward_max           | 38.5327     |\n",
      "|    reward_mean          | 0.15795353  |\n",
      "|    reward_min           | -51.693203  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 88.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038214266 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.00635    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | 1.8271856   |\n",
      "|    reward_max           | 16.297588   |\n",
      "|    reward_mean          | 0.16990562  |\n",
      "|    reward_min           | -16.22201   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 85.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01997531  |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.00219    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    reward               | 0.116753295 |\n",
      "|    reward_max           | 42.61699    |\n",
      "|    reward_mean          | 0.067568704 |\n",
      "|    reward_min           | -51.95418   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 32.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026238441 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00283    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.4        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    reward               | 0.4531729   |\n",
      "|    reward_max           | 42.68852    |\n",
      "|    reward_mean          | 0.13865049  |\n",
      "|    reward_min           | -51.322445  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 59.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031208593 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | -0.00325    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 75.9        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    reward               | 0.5471338   |\n",
      "|    reward_max           | 40.00671    |\n",
      "|    reward_mean          | 0.1385368   |\n",
      "|    reward_min           | -53.78421   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 79.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023607682 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.00296     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.6        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    reward               | -1.3278011  |\n",
      "|    reward_max           | 8.787884    |\n",
      "|    reward_mean          | 0.12609048  |\n",
      "|    reward_min           | -9.259638   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 82.7        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3907869.52\n",
      "total_reward: 2907869.52\n",
      "total_cost: 306576.64\n",
      "total_trades: 79254\n",
      "Sharpe: 0.744\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017259507 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | -0.0309     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.33        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    reward               | -0.21160118 |\n",
      "|    reward_max           | 33.170395   |\n",
      "|    reward_mean          | 0.08246422  |\n",
      "|    reward_min           | -48.737022  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 20.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029806945 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | -0.012      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 51.7        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    reward               | 0.6201361   |\n",
      "|    reward_max           | 49.25616    |\n",
      "|    reward_mean          | 0.2056936   |\n",
      "|    reward_min           | -79.23318   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 97.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 213        |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02680018 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.8      |\n",
      "|    explained_variance   | 0.00816    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 133        |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    reward               | -0.6959065 |\n",
      "|    reward_max           | 18.810564  |\n",
      "|    reward_mean          | 0.1510348  |\n",
      "|    reward_min           | -19.281345 |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 164        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024224356 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.00327     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.4        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    reward               | 4.304237    |\n",
      "|    reward_max           | 32.930977   |\n",
      "|    reward_mean          | 0.049483724 |\n",
      "|    reward_min           | -52.378075  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 36.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019824907 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.0108      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 32.2        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    reward               | -0.46534845 |\n",
      "|    reward_max           | 43.55936    |\n",
      "|    reward_mean          | 0.16626744  |\n",
      "|    reward_min           | -72.489     |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 96.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038867865 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | -0.0165     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 49.2        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00791    |\n",
      "|    reward               | 0.8024148   |\n",
      "|    reward_max           | 48.90541    |\n",
      "|    reward_mean          | 0.18117353  |\n",
      "|    reward_min           | -88.83815   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013140258 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | -0.0246     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 203         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00898    |\n",
      "|    reward               | 0.5761032   |\n",
      "|    reward_max           | 11.495887   |\n",
      "|    reward_mean          | 0.15764296  |\n",
      "|    reward_min           | -12.430653  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 324         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 289         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027639568 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | -0.0392     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.1        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00724    |\n",
      "|    reward               | -1.1421564  |\n",
      "|    reward_max           | 58.513268   |\n",
      "|    reward_mean          | 0.1822405   |\n",
      "|    reward_min           | -93.00525   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 28.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029643398 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | 0.00581     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00924    |\n",
      "|    reward               | -0.33667916 |\n",
      "|    reward_max           | 51.404938   |\n",
      "|    reward_mean          | 0.1923523   |\n",
      "|    reward_min           | -82.66931   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 313         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025926566 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.00405     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 89.5        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    reward               | -12.104189  |\n",
      "|    reward_max           | 17.620363   |\n",
      "|    reward_mean          | 0.1847857   |\n",
      "|    reward_min           | -25.65205   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 228         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 133        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 338        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02653109 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.1      |\n",
      "|    explained_variance   | 0.00606    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 32.6       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.00971   |\n",
      "|    reward               | 3.8770459  |\n",
      "|    reward_max           | 56.11274   |\n",
      "|    reward_mean          | 0.18573354 |\n",
      "|    reward_min           | -92.11353  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 70.8       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 354          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.027428601  |\n",
      "|    clip_fraction        | 0.272        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.2        |\n",
      "|    explained_variance   | -0.0021      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 49.6         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0171      |\n",
      "|    reward               | -0.120253876 |\n",
      "|    reward_max           | 66.24842     |\n",
      "|    reward_mean          | 0.25502974   |\n",
      "|    reward_min           | -108.01342   |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 208          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 370         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023787368 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | -0.0227     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 213         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00858    |\n",
      "|    reward               | 6.2521667   |\n",
      "|    reward_max           | 52.888702   |\n",
      "|    reward_mean          | 0.19195087  |\n",
      "|    reward_min           | -80.78182   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 313         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 386        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02668569 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.2      |\n",
      "|    explained_variance   | -0.00162   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 129        |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.00993   |\n",
      "|    reward               | 2.1696386  |\n",
      "|    reward_max           | 18.042498  |\n",
      "|    reward_mean          | 0.16825454 |\n",
      "|    reward_min           | -41.693943 |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 176        |\n",
      "----------------------------------------\n",
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6489099.73\n",
      "total_reward: 5489099.73\n",
      "total_cost: 286338.32\n",
      "total_trades: 76629\n",
      "Sharpe: 0.959\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 402         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026517943 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0105      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    reward               | 1.295671    |\n",
      "|    reward_max           | 57.17519    |\n",
      "|    reward_mean          | 0.17942283  |\n",
      "|    reward_min           | -91.70116   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 41.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 418        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03444373 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.4      |\n",
      "|    explained_variance   | 0.00458    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 175        |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    reward               | 0.18362889 |\n",
      "|    reward_max           | 55.177296  |\n",
      "|    reward_mean          | 0.23098221 |\n",
      "|    reward_min           | -94.71607  |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 278        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 434        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02116526 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.4      |\n",
      "|    explained_variance   | 0.00259    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 170        |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.00623   |\n",
      "|    reward               | -4.4425945 |\n",
      "|    reward_max           | 18.795645  |\n",
      "|    reward_mean          | 0.2229078  |\n",
      "|    reward_min           | -28.713352 |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 208        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 449        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02010878 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.5      |\n",
      "|    explained_variance   | 0.0787     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 29         |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    reward               | 1.7965059  |\n",
      "|    reward_max           | 60.719536  |\n",
      "|    reward_mean          | 0.159713   |\n",
      "|    reward_min           | -98.45733  |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 60.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024442703 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.0208      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 101         |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    reward               | 1.7433106   |\n",
      "|    reward_max           | 74.80793    |\n",
      "|    reward_mean          | 0.27149594  |\n",
      "|    reward_min           | -121.39452  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 227         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 480         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023355417 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.00942     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 240         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 3.0343924   |\n",
      "|    reward_max           | 23.814274   |\n",
      "|    reward_mean          | 0.23506747  |\n",
      "|    reward_min           | -27.201078  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 361         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 497         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025801506 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00948    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 59.1        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    reward               | -0.29710445 |\n",
      "|    reward_max           | 58.071373   |\n",
      "|    reward_mean          | 0.1613537   |\n",
      "|    reward_min           | -94.31206   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 131        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 512        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04849783 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.6      |\n",
      "|    explained_variance   | 0.00596    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 93.5       |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.00677   |\n",
      "|    reward               | -0.2540203 |\n",
      "|    reward_max           | 62.5103    |\n",
      "|    reward_mean          | 0.21707863 |\n",
      "|    reward_min           | -97.73678  |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 172        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 528         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029688891 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0368      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 101         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    reward               | 1.9620143   |\n",
      "|    reward_max           | 59.83016    |\n",
      "|    reward_mean          | 0.23669644  |\n",
      "|    reward_min           | -94.02635   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 254         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 545         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018099748 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0216      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 158         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    reward               | -0.27231413 |\n",
      "|    reward_max           | 14.964804   |\n",
      "|    reward_mean          | 0.20047283  |\n",
      "|    reward_min           | -10.794531  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 228         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047620885 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.3        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00829    |\n",
      "|    reward               | -4.944815   |\n",
      "|    reward_max           | 55.852993   |\n",
      "|    reward_mean          | 0.109033644 |\n",
      "|    reward_min           | -92.64342   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 43.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 581         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014168418 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | -0.00411    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 206         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    reward               | -0.55621886 |\n",
      "|    reward_max           | 55.963577   |\n",
      "|    reward_mean          | 0.19668825  |\n",
      "|    reward_min           | -83.92503   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 355         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 598         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016095858 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.028       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 69.3        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    reward               | -20.388096  |\n",
      "|    reward_max           | 32.853916   |\n",
      "|    reward_mean          | 0.2035332   |\n",
      "|    reward_min           | -27.590319  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 239         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 615         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023572292 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0278      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.8        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00209    |\n",
      "|    reward               | -2.8638334  |\n",
      "|    reward_max           | 56.64581    |\n",
      "|    reward_mean          | 0.14144169  |\n",
      "|    reward_min           | -80.56275   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 99.2        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4342983.52\n",
      "total_reward: 3342983.52\n",
      "total_cost: 255988.92\n",
      "total_trades: 74172\n",
      "Sharpe: 0.782\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 632         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020170074 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0252      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 135         |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -1.2008691  |\n",
      "|    reward_max           | 38.069405   |\n",
      "|    reward_mean          | 0.14694066  |\n",
      "|    reward_min           | -57.267323  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 650         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019686472 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | -0.0292     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 78          |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    reward               | 0.48685637  |\n",
      "|    reward_max           | 53.356884   |\n",
      "|    reward_mean          | 0.17209375  |\n",
      "|    reward_min           | -81.37923   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 667         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018595573 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.00493     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 147         |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | -1.4242402  |\n",
      "|    reward_max           | 13.882841   |\n",
      "|    reward_mean          | 0.21583939  |\n",
      "|    reward_min           | -14.404863  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 305         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022648625 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.071       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.5        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    reward               | 1.2634395   |\n",
      "|    reward_max           | 75.98158    |\n",
      "|    reward_mean          | 0.2496363   |\n",
      "|    reward_min           | -130.23955  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 44.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01754031  |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.0296      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 62.3        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    reward               | -0.02674362 |\n",
      "|    reward_max           | 68.44443    |\n",
      "|    reward_mean          | 0.25192675  |\n",
      "|    reward_min           | -112.20251  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 383         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 718         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022987312 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.0331      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 98.8        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0039     |\n",
      "|    reward               | -4.1477065  |\n",
      "|    reward_max           | 24.9251     |\n",
      "|    reward_mean          | 0.262728    |\n",
      "|    reward_min           | -38.65963   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 377         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 734         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027939875 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.0821      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 62.2        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00833    |\n",
      "|    reward               | -7.1884885  |\n",
      "|    reward_max           | 73.01       |\n",
      "|    reward_mean          | 0.20053425  |\n",
      "|    reward_min           | -126.889824 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 750         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028969098 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.044       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 140         |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00748    |\n",
      "|    reward               | 1.1802641   |\n",
      "|    reward_max           | 77.15455    |\n",
      "|    reward_mean          | 0.308327    |\n",
      "|    reward_min           | -140.21631  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 363         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 765         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023178808 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.0745      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 80          |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00572    |\n",
      "|    reward               | 24.36427    |\n",
      "|    reward_max           | 75.46587    |\n",
      "|    reward_mean          | 0.2467677   |\n",
      "|    reward_min           | -132.53226  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 353         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 781        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02081166 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.3      |\n",
      "|    explained_variance   | 0.0237     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 65.2       |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    reward               | -2.9368627 |\n",
      "|    reward_max           | 28.234802  |\n",
      "|    reward_mean          | 0.21597491 |\n",
      "|    reward_min           | -60.189205 |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 350        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 796         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03190819  |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    reward               | -0.24673387 |\n",
      "|    reward_max           | 62.676056   |\n",
      "|    reward_mean          | 0.24786156  |\n",
      "|    reward_min           | -92.45365   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 58.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 812         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060149238 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0877      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 78.8        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00311    |\n",
      "|    reward               | -0.51327896 |\n",
      "|    reward_max           | 49.972183   |\n",
      "|    reward_mean          | 0.22185332  |\n",
      "|    reward_min           | -73.53323   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 230         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 828        |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03241379 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.5      |\n",
      "|    explained_variance   | 0.0776     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 77         |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.00147   |\n",
      "|    reward               | -1.7730966 |\n",
      "|    reward_max           | 13.6012535 |\n",
      "|    reward_mean          | 0.13487636 |\n",
      "|    reward_min           | -20.822248 |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 173        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 844         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016703468 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    reward               | 1.4504696   |\n",
      "|    reward_max           | 37.009552   |\n",
      "|    reward_mean          | 0.14022097  |\n",
      "|    reward_min           | -56.556454  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6039054.42\n",
      "total_reward: 5039054.42\n",
      "total_cost: 258780.79\n",
      "total_trades: 74908\n",
      "Sharpe: 0.860\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 860         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029580427 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.0927      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 94.1        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00998    |\n",
      "|    reward               | 1.2401416   |\n",
      "|    reward_max           | 45.78427    |\n",
      "|    reward_mean          | 0.19271632  |\n",
      "|    reward_min           | -67.66801   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 876        |\n",
      "|    total_timesteps      | 112640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02572392 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.6      |\n",
      "|    explained_variance   | 0.0949     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 54.4       |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.00487   |\n",
      "|    reward               | 8.224691   |\n",
      "|    reward_max           | 34.274597  |\n",
      "|    reward_mean          | 0.22000417 |\n",
      "|    reward_min           | -29.871176 |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 166        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 892         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014530247 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.0665      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 55.9        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00771    |\n",
      "|    reward               | 0.72506374  |\n",
      "|    reward_max           | 57.074806   |\n",
      "|    reward_mean          | 0.17231885  |\n",
      "|    reward_min           | -84.13855   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 908         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03800489  |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 80.4        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    reward               | -0.75280493 |\n",
      "|    reward_max           | 52.517887   |\n",
      "|    reward_mean          | 0.18703811  |\n",
      "|    reward_min           | -79.94277   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 155         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 924         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017263826 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 109         |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0085     |\n",
      "|    reward               | 0.45148957  |\n",
      "|    reward_max           | 54.815422   |\n",
      "|    reward_mean          | 0.25260028  |\n",
      "|    reward_min           | -94.150925  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 171         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 940         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017942717 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0634      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48.7        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00327    |\n",
      "|    reward               | 0.68799496  |\n",
      "|    reward_max           | 9.982578    |\n",
      "|    reward_mean          | 0.11877526  |\n",
      "|    reward_min           | -11.9930725 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 192         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 958          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.06700301   |\n",
      "|    clip_fraction        | 0.323        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.7        |\n",
      "|    explained_variance   | 0.261        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 10.6         |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    reward               | 0.0020918408 |\n",
      "|    reward_max           | 39.343517    |\n",
      "|    reward_mean          | 0.14552738   |\n",
      "|    reward_min           | -56.33299    |\n",
      "|    std                  | 1.1          |\n",
      "|    value_loss           | 22.9         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 977        |\n",
      "|    total_timesteps      | 124928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03199534 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.8      |\n",
      "|    explained_variance   | 0.0205     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 49.2       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    reward               | 0.8189754  |\n",
      "|    reward_max           | 56.955647  |\n",
      "|    reward_mean          | 0.2205297  |\n",
      "|    reward_min           | -70.7757   |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 98.1       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 997        |\n",
      "|    total_timesteps      | 126976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02161377 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.9      |\n",
      "|    explained_variance   | 0.0142     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 103        |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0063    |\n",
      "|    reward               | 6.229455   |\n",
      "|    reward_max           | 21.40705   |\n",
      "|    reward_mean          | 0.14236651 |\n",
      "|    reward_min           | -19.518902 |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 177        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 1013        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017207097 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0246      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    reward               | 4.1572943   |\n",
      "|    reward_max           | 38.229862   |\n",
      "|    reward_mean          | 0.13795684  |\n",
      "|    reward_min           | -59.76774   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 42.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 1030       |\n",
      "|    total_timesteps      | 131072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03212037 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.9      |\n",
      "|    explained_variance   | 0.0511     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 37.8       |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    reward               | -0.4169185 |\n",
      "|    reward_max           | 52.135765  |\n",
      "|    reward_mean          | 0.19517846 |\n",
      "|    reward_min           | -89.01352  |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 69.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 1045        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02561307  |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.0801      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48.9        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00877    |\n",
      "|    reward               | -0.81001604 |\n",
      "|    reward_max           | 56.12844    |\n",
      "|    reward_mean          | 0.2573795   |\n",
      "|    reward_min           | -91.35801   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 145         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 1061        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033708576 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.022       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.6        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00682    |\n",
      "|    reward               | 3.6207452   |\n",
      "|    reward_max           | 14.078249   |\n",
      "|    reward_mean          | 0.1695956   |\n",
      "|    reward_min           | -13.384972  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 174         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 1077        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034622863 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.8         |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00845    |\n",
      "|    reward               | -2.2056837  |\n",
      "|    reward_max           | 62.30212    |\n",
      "|    reward_mean          | 0.19930193  |\n",
      "|    reward_min           | -94.252144  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5991011.75\n",
      "total_reward: 4991011.75\n",
      "total_cost: 274859.77\n",
      "total_trades: 75655\n",
      "Sharpe: 0.997\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 1092        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025456317 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 87.3        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00501    |\n",
      "|    reward               | 0.23171769  |\n",
      "|    reward_max           | 43.356995   |\n",
      "|    reward_mean          | 0.19722065  |\n",
      "|    reward_min           | -69.46509   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 164         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 1108        |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022458324 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 47.9        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | -4.4585476  |\n",
      "|    reward_max           | 12.150805   |\n",
      "|    reward_mean          | 0.18118678  |\n",
      "|    reward_min           | -21.000677  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1125        |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023270866 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    reward               | 1.7209831   |\n",
      "|    reward_max           | 42.176613   |\n",
      "|    reward_mean          | 0.11451244  |\n",
      "|    reward_min           | -58.613     |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 32          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 1141        |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033147365 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.0685      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 45.4        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | 0.67800105  |\n",
      "|    reward_max           | 50.768303   |\n",
      "|    reward_mean          | 0.19062936  |\n",
      "|    reward_min           | -76.2351    |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 95          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 1156        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041699037 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.0994      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 65.3        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | 0.00199     |\n",
      "|    reward               | -24.499651  |\n",
      "|    reward_max           | 25.190575   |\n",
      "|    reward_mean          | 0.2108512   |\n",
      "|    reward_min           | -24.499651  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 1171        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024722908 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 56          |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    reward               | -0.3144836  |\n",
      "|    reward_max           | 54.79164    |\n",
      "|    reward_mean          | 0.13203491  |\n",
      "|    reward_min           | -83.62523   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 1187        |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033216808 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 46.4        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    reward               | -1.5533936  |\n",
      "|    reward_max           | 56.938858   |\n",
      "|    reward_mean          | 0.21221523  |\n",
      "|    reward_min           | -72.74697   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 75.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 1202        |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042249754 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.0514      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.2        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | 0.000878    |\n",
      "|    reward               | 1.5596068   |\n",
      "|    reward_max           | 45.3377     |\n",
      "|    reward_mean          | 0.17695099  |\n",
      "|    reward_min           | -53.07508   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 1218        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036660574 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.0221      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 118         |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00433    |\n",
      "|    reward               | -2.2551403  |\n",
      "|    reward_max           | 11.816465   |\n",
      "|    reward_mean          | 0.16139187  |\n",
      "|    reward_min           | -20.225613  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 1233        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046157494 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.6        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    reward               | 0.17316826  |\n",
      "|    reward_max           | 50.719734   |\n",
      "|    reward_mean          | 0.169152    |\n",
      "|    reward_min           | -63.346546  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 29.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 1248        |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032968886 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.0785      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 89.5        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    reward               | 3.485969    |\n",
      "|    reward_max           | 53.724808   |\n",
      "|    reward_mean          | 0.22442324  |\n",
      "|    reward_min           | -74.39622   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 1264       |\n",
      "|    total_timesteps      | 161792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03277295 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.8      |\n",
      "|    explained_variance   | 0.0375     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 46.1       |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | 0.00194    |\n",
      "|    reward               | -7.2279615 |\n",
      "|    reward_max           | 30.011826  |\n",
      "|    reward_mean          | 0.25041008 |\n",
      "|    reward_min           | -25.877106 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 141        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1279        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040800005 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 45.3        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.00232     |\n",
      "|    reward               | 1.1315067   |\n",
      "|    reward_max           | 65.92478    |\n",
      "|    reward_mean          | 0.167642    |\n",
      "|    reward_min           | -95.54219   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 68          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 1295        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030544532 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.0361      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 43          |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00447    |\n",
      "|    reward               | -0.6775047  |\n",
      "|    reward_max           | 56.279617   |\n",
      "|    reward_mean          | 0.22300152  |\n",
      "|    reward_min           | -86.67931   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 155         |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7597427.76\n",
      "total_reward: 6597427.76\n",
      "total_cost: 249572.04\n",
      "total_trades: 72980\n",
      "Sharpe: 1.058\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1310        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023217637 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.153       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 52.7        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00501    |\n",
      "|    reward               | -0.36470747 |\n",
      "|    reward_max           | 61.94314    |\n",
      "|    reward_mean          | 0.2963007   |\n",
      "|    reward_min           | -96.45697   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 153         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 1326       |\n",
      "|    total_timesteps      | 169984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03080164 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.9      |\n",
      "|    explained_variance   | 0.0587     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 74.7       |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | 0.00496    |\n",
      "|    reward               | -1.0011325 |\n",
      "|    reward_max           | 13.628373  |\n",
      "|    reward_mean          | 0.18175177 |\n",
      "|    reward_min           | -12.702349 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 185        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 1341        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039957304 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | 0.00178     |\n",
      "|    reward               | -0.5591772  |\n",
      "|    reward_max           | 66.274925   |\n",
      "|    reward_mean          | 0.2287289   |\n",
      "|    reward_min           | -115.37934  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 27.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1356        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02539726  |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 140         |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00462    |\n",
      "|    reward               | -0.08045235 |\n",
      "|    reward_max           | 60.974415   |\n",
      "|    reward_mean          | 0.25790554  |\n",
      "|    reward_min           | -109.367195 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 258         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 1372       |\n",
      "|    total_timesteps      | 176128     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02680711 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.1      |\n",
      "|    explained_variance   | 0.115      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 128        |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.00886   |\n",
      "|    reward               | 1.6744342  |\n",
      "|    reward_max           | 30.583494  |\n",
      "|    reward_mean          | 0.23836811 |\n",
      "|    reward_min           | -31.147581 |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 262        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 1388       |\n",
      "|    total_timesteps      | 178176     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03299153 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.1      |\n",
      "|    explained_variance   | 0.108      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 60.4       |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.00611   |\n",
      "|    reward               | 3.1305227  |\n",
      "|    reward_max           | 60.453224  |\n",
      "|    reward_mean          | 0.2208563  |\n",
      "|    reward_min           | -104.27305 |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 96.8       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 88         |\n",
      "|    time_elapsed         | 1403       |\n",
      "|    total_timesteps      | 180224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02651218 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.2      |\n",
      "|    explained_variance   | 0.0924     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 90.1       |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    reward               | -1.0534644 |\n",
      "|    reward_max           | 55.324837  |\n",
      "|    reward_mean          | 0.21186274 |\n",
      "|    reward_min           | -100.04835 |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 181        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 1419         |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.032023333  |\n",
      "|    clip_fraction        | 0.296        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -45.2        |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 40.5         |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00377     |\n",
      "|    reward               | -0.111171834 |\n",
      "|    reward_max           | 60.67495     |\n",
      "|    reward_mean          | 0.2894601    |\n",
      "|    reward_min           | -105.987     |\n",
      "|    std                  | 1.15         |\n",
      "|    value_loss           | 236          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1435        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032705765 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.0111      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 138         |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.000449   |\n",
      "|    reward               | 1.612778    |\n",
      "|    reward_max           | 17.797497   |\n",
      "|    reward_mean          | 0.21124868  |\n",
      "|    reward_min           | -19.202879  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 264         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1450        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03507888  |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.2        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00553    |\n",
      "|    reward               | 0.093999594 |\n",
      "|    reward_max           | 77.28453    |\n",
      "|    reward_mean          | 0.27868968  |\n",
      "|    reward_min           | -138.83601  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1466        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017347362 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 144         |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00509    |\n",
      "|    reward               | -1.4932374  |\n",
      "|    reward_max           | 59.879562   |\n",
      "|    reward_mean          | 0.2624404   |\n",
      "|    reward_min           | -106.45728  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 410         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1481        |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022136271 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.0897      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 148         |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | 0.00414     |\n",
      "|    reward               | -5.6202564  |\n",
      "|    reward_max           | 22.009575   |\n",
      "|    reward_mean          | 0.2396854   |\n",
      "|    reward_min           | -33.42307   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 265         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 1496       |\n",
      "|    total_timesteps      | 192512     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0327894  |\n",
      "|    clip_fraction        | 0.346      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.4      |\n",
      "|    explained_variance   | 0.0975     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 38.3       |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0011    |\n",
      "|    reward               | -1.4110928 |\n",
      "|    reward_max           | 60.25435   |\n",
      "|    reward_mean          | 0.21301514 |\n",
      "|    reward_min           | -106.21245 |\n",
      "|    std                  | 1.16       |\n",
      "|    value_loss           | 83.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1512        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030940503 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 195         |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00894    |\n",
      "|    reward               | -0.52363104 |\n",
      "|    reward_max           | 51.83565    |\n",
      "|    reward_mean          | 0.18305625  |\n",
      "|    reward_min           | -87.1999    |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 219         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1527        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030808013 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 163         |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.00407     |\n",
      "|    reward               | 3.408531    |\n",
      "|    reward_max           | 24.396141   |\n",
      "|    reward_mean          | 0.23155718  |\n",
      "|    reward_min           | -27.662237  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 193         |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6252021.19\n",
      "total_reward: 5252021.19\n",
      "total_cost: 220647.44\n",
      "total_trades: 69669\n",
      "Sharpe: 1.009\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1542        |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026341986 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.0464      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 37.1        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | 0.000657    |\n",
      "|    reward               | -1.7868104  |\n",
      "|    reward_max           | 46.985874   |\n",
      "|    reward_mean          | 0.19537422  |\n",
      "|    reward_min           | -62.1026    |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 98         |\n",
      "|    time_elapsed         | 1558       |\n",
      "|    total_timesteps      | 200704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04331583 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.6      |\n",
      "|    explained_variance   | 0.222      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 54.8       |\n",
      "|    n_updates            | 970        |\n",
      "|    policy_gradient_loss | -0.000906  |\n",
      "|    reward               | -1.07169   |\n",
      "|    reward_max           | 65.29409   |\n",
      "|    reward_mean          | 0.25357336 |\n",
      "|    reward_min           | -110.35988 |\n",
      "|    std                  | 1.17       |\n",
      "|    value_loss           | 93.8       |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5655972.36\n",
      "total_reward: 4655972.36\n",
      "total_cost: 4903.82\n",
      "total_trades: 39466\n",
      "Sharpe: 1.038\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 50       |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 11572    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 34.8     |\n",
      "|    critic_loss     | 23.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11471    |\n",
      "|    reward          | 9.553164 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 430      |\n",
      "|    total_timesteps | 23144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.1     |\n",
      "|    critic_loss     | 172      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23043    |\n",
      "|    reward          | 9.553164 |\n",
      "---------------------------------\n",
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6043982.18\n",
      "total_reward: 5043982.18\n",
      "total_cost: 999.00\n",
      "total_trades: 37596\n",
      "Sharpe: 0.911\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 633      |\n",
      "|    total_timesteps | 34716    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 36.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 34615    |\n",
      "|    reward          | 9.553164 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 838      |\n",
      "|    total_timesteps | 46288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 46187    |\n",
      "|    reward          | 9.553164 |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6223084.42\n",
      "total_reward: 5223084.42\n",
      "total_cost: 87787.30\n",
      "total_trades: 46937\n",
      "Sharpe: 0.920\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 48        |\n",
      "|    time_elapsed    | 238       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.94e+03  |\n",
      "|    critic_loss     | 232       |\n",
      "|    ent_coef        | 0.275     |\n",
      "|    ent_coef_loss   | 128       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 6.5337534 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 46        |\n",
      "|    time_elapsed    | 493       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 861       |\n",
      "|    critic_loss     | 137       |\n",
      "|    ent_coef        | 0.11      |\n",
      "|    ent_coef_loss   | -96.4     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 23043     |\n",
      "|    reward          | 1.7469625 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 47        |\n",
      "|    time_elapsed    | 734       |\n",
      "|    total_timesteps | 34716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 365       |\n",
      "|    critic_loss     | 21.9      |\n",
      "|    ent_coef        | 0.0356    |\n",
      "|    ent_coef_loss   | -109      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 34615     |\n",
      "|    reward          | 3.8252249 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5470282.77\n",
      "total_reward: 4470282.77\n",
      "total_cost: 5882.41\n",
      "total_trades: 46310\n",
      "Sharpe: 0.858\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 977      |\n",
      "|    total_timesteps | 46288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 177      |\n",
      "|    critic_loss     | 72       |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | -96.5    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 46187    |\n",
      "|    reward          | 4.993111 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 47        |\n",
      "|    time_elapsed    | 1222      |\n",
      "|    total_timesteps | 57860     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 92.6      |\n",
      "|    critic_loss     | 12        |\n",
      "|    ent_coef        | 0.00392   |\n",
      "|    ent_coef_loss   | -60       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 57759     |\n",
      "|    reward          | 5.2938304 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5734320.77\n",
      "total_reward: 4734320.77\n",
      "total_cost: 2055.48\n",
      "total_trades: 47057\n",
      "Sharpe: 0.897\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 1465     |\n",
      "|    total_timesteps | 69432    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 61.7     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.00215  |\n",
      "|    ent_coef_loss   | 4.22     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 69331    |\n",
      "|    reward          | 5.358457 |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
